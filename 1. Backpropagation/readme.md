# BackPropagation
It is an algorithm to train the NN(neural networks), Means this algorithm tries to find the best optimum values of weights and bias for given data on which our model gives best results.

It is an efficient application of the chain rule to neural networks. Backpropagation computes the gradient of a loss function with respect to the weights of the network for a single inputâ€“output example, and does so efficiently, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this can be derived through dynamic programming.

![neural_network](https://github.com/user-attachments/assets/d40cad0d-3cef-43eb-a94c-527da256b9b4)
