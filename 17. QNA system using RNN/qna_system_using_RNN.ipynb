{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7984ab-b8f9-46cc-b331-d0fe03a3f92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8da3e722-ec0b-4dd4-aa3d-370e757f0573",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"100_Unique_QA_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56492f37-0897-49d6-9466-5fb468846825",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>Paris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the capital of Germany?</td>\n",
       "      <td>Berlin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who wrote 'To Kill a Mockingbird'?</td>\n",
       "      <td>Harper-Lee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the largest planet in our solar system?</td>\n",
       "      <td>Jupiter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the boiling point of water in Celsius?</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          question      answer\n",
       "0                   What is the capital of France?       Paris\n",
       "1                  What is the capital of Germany?      Berlin\n",
       "2               Who wrote 'To Kill a Mockingbird'?  Harper-Lee\n",
       "3  What is the largest planet in our solar system?     Jupiter\n",
       "4   What is the boiling point of water in Celsius?         100"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d22a716-5bf3-4d93-bf6f-be783810efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "def tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\"?\", '')\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf78cf7-6f31-4090-93ed-53c07679eaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what', 'is', 'the', 'capital', 'of', 'france']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(\"What is the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebb6a1c7-6a66-44bf-a507-e970e75a39a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab\n",
    "vocab = {\"<UNK>\" : 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a788a3f0-3aa5-4e2f-88b1-cf44acf835f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1025935b-7d39-4e16-a2ba-9edee83fe3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(row):\n",
    "\n",
    "    tokenized_question = tokenize(row[\"question\"])\n",
    "    tokenized_answer = tokenize(row[\"answer\"])\n",
    "\n",
    "    merged_tokens = tokenized_question + tokenized_answer\n",
    "\n",
    "    for token in merged_tokens:\n",
    "\n",
    "        if token not in vocab:\n",
    "            vocab[token] = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ede7e5c8-5732-4f69-8f18-ae45d5db9c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     None\n",
       "1     None\n",
       "2     None\n",
       "3     None\n",
       "4     None\n",
       "      ... \n",
       "85    None\n",
       "86    None\n",
       "87    None\n",
       "88    None\n",
       "89    None\n",
       "Length: 90, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(build_vocab, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7f4d095-6361-4641-9c6a-73b1829fc248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'what': 1,\n",
       " 'is': 2,\n",
       " 'the': 3,\n",
       " 'capital': 4,\n",
       " 'of': 5,\n",
       " 'france': 6,\n",
       " 'paris': 7,\n",
       " 'germany': 8,\n",
       " 'berlin': 9,\n",
       " 'who': 10,\n",
       " 'wrote': 11,\n",
       " 'to': 12,\n",
       " 'kill': 13,\n",
       " 'a': 14,\n",
       " 'mockingbird': 15,\n",
       " 'harper-lee': 16,\n",
       " 'largest': 17,\n",
       " 'planet': 18,\n",
       " 'in': 19,\n",
       " 'our': 20,\n",
       " 'solar': 21,\n",
       " 'system': 22,\n",
       " 'jupiter': 23,\n",
       " 'boiling': 24,\n",
       " 'point': 25,\n",
       " 'water': 26,\n",
       " 'celsius': 27,\n",
       " '100': 28,\n",
       " 'painted': 29,\n",
       " 'mona': 30,\n",
       " 'lisa': 31,\n",
       " 'leonardo-da-vinci': 32,\n",
       " 'square': 33,\n",
       " 'root': 34,\n",
       " '64': 35,\n",
       " '8': 36,\n",
       " 'chemical': 37,\n",
       " 'symbol': 38,\n",
       " 'for': 39,\n",
       " 'gold': 40,\n",
       " 'au': 41,\n",
       " 'which': 42,\n",
       " 'year': 43,\n",
       " 'did': 44,\n",
       " 'world': 45,\n",
       " 'war': 46,\n",
       " 'ii': 47,\n",
       " 'end': 48,\n",
       " '1945': 49,\n",
       " 'longest': 50,\n",
       " 'river': 51,\n",
       " 'nile': 52,\n",
       " 'japan': 53,\n",
       " 'tokyo': 54,\n",
       " 'developed': 55,\n",
       " 'theory': 56,\n",
       " 'relativity': 57,\n",
       " 'albert-einstein': 58,\n",
       " 'freezing': 59,\n",
       " 'fahrenheit': 60,\n",
       " '32': 61,\n",
       " 'known': 62,\n",
       " 'as': 63,\n",
       " 'red': 64,\n",
       " 'mars': 65,\n",
       " 'author': 66,\n",
       " '1984': 67,\n",
       " 'george-orwell': 68,\n",
       " 'currency': 69,\n",
       " 'united': 70,\n",
       " 'kingdom': 71,\n",
       " 'pound': 72,\n",
       " 'india': 73,\n",
       " 'delhi': 74,\n",
       " 'discovered': 75,\n",
       " 'gravity': 76,\n",
       " 'newton': 77,\n",
       " 'how': 78,\n",
       " 'many': 79,\n",
       " 'continents': 80,\n",
       " 'are': 81,\n",
       " 'there': 82,\n",
       " 'on': 83,\n",
       " 'earth': 84,\n",
       " '7': 85,\n",
       " 'gas': 86,\n",
       " 'do': 87,\n",
       " 'plants': 88,\n",
       " 'use': 89,\n",
       " 'photosynthesis': 90,\n",
       " 'co2': 91,\n",
       " 'smallest': 92,\n",
       " 'prime': 93,\n",
       " 'number': 94,\n",
       " '2': 95,\n",
       " 'invented': 96,\n",
       " 'telephone': 97,\n",
       " 'alexander-graham-bell': 98,\n",
       " 'australia': 99,\n",
       " 'canberra': 100,\n",
       " 'ocean': 101,\n",
       " 'pacific-ocean': 102,\n",
       " 'speed': 103,\n",
       " 'light': 104,\n",
       " 'vacuum': 105,\n",
       " '299,792,458m/s': 106,\n",
       " 'language': 107,\n",
       " 'spoken': 108,\n",
       " 'brazil': 109,\n",
       " 'portuguese': 110,\n",
       " 'penicillin': 111,\n",
       " 'alexander-fleming': 112,\n",
       " 'canada': 113,\n",
       " 'ottawa': 114,\n",
       " 'mammal': 115,\n",
       " 'whale': 116,\n",
       " 'element': 117,\n",
       " 'has': 118,\n",
       " 'atomic': 119,\n",
       " '1': 120,\n",
       " 'hydrogen': 121,\n",
       " 'tallest': 122,\n",
       " 'mountain': 123,\n",
       " 'everest': 124,\n",
       " 'city': 125,\n",
       " 'big': 126,\n",
       " 'apple': 127,\n",
       " 'newyork': 128,\n",
       " 'planets': 129,\n",
       " 'starry': 130,\n",
       " 'night': 131,\n",
       " 'vangogh': 132,\n",
       " 'formula': 133,\n",
       " 'h2o': 134,\n",
       " 'italy': 135,\n",
       " 'rome': 136,\n",
       " 'country': 137,\n",
       " 'famous': 138,\n",
       " 'sushi': 139,\n",
       " 'was': 140,\n",
       " 'first': 141,\n",
       " 'person': 142,\n",
       " 'step': 143,\n",
       " 'moon': 144,\n",
       " 'armstrong': 145,\n",
       " 'main': 146,\n",
       " 'ingredient': 147,\n",
       " 'guacamole': 148,\n",
       " 'avocado': 149,\n",
       " 'sides': 150,\n",
       " 'does': 151,\n",
       " 'hexagon': 152,\n",
       " 'have': 153,\n",
       " '6': 154,\n",
       " 'china': 155,\n",
       " 'yuan': 156,\n",
       " 'pride': 157,\n",
       " 'and': 158,\n",
       " 'prejudice': 159,\n",
       " 'jane-austen': 160,\n",
       " 'iron': 161,\n",
       " 'fe': 162,\n",
       " 'hardest': 163,\n",
       " 'natural': 164,\n",
       " 'substance': 165,\n",
       " 'diamond': 166,\n",
       " 'continent': 167,\n",
       " 'by': 168,\n",
       " 'area': 169,\n",
       " 'asia': 170,\n",
       " 'president': 171,\n",
       " 'states': 172,\n",
       " 'george-washington': 173,\n",
       " 'bird': 174,\n",
       " 'its': 175,\n",
       " 'ability': 176,\n",
       " 'mimic': 177,\n",
       " 'sounds': 178,\n",
       " 'parrot': 179,\n",
       " 'longest-running': 180,\n",
       " 'animated': 181,\n",
       " 'tv': 182,\n",
       " 'show': 183,\n",
       " 'simpsons': 184,\n",
       " 'vaticancity': 185,\n",
       " 'most': 186,\n",
       " 'moons': 187,\n",
       " 'saturn': 188,\n",
       " 'romeo': 189,\n",
       " 'juliet': 190,\n",
       " 'shakespeare': 191,\n",
       " 'earths': 192,\n",
       " 'atmosphere': 193,\n",
       " 'nitrogen': 194,\n",
       " 'bones': 195,\n",
       " 'adult': 196,\n",
       " 'human': 197,\n",
       " 'body': 198,\n",
       " '206': 199,\n",
       " 'metal': 200,\n",
       " 'liquid': 201,\n",
       " 'at': 202,\n",
       " 'room': 203,\n",
       " 'temperature': 204,\n",
       " 'mercury': 205,\n",
       " 'russia': 206,\n",
       " 'moscow': 207,\n",
       " 'electricity': 208,\n",
       " 'benjamin-franklin': 209,\n",
       " 'second-largest': 210,\n",
       " 'land': 211,\n",
       " 'color': 212,\n",
       " 'ripe': 213,\n",
       " 'banana': 214,\n",
       " 'yellow': 215,\n",
       " 'month': 216,\n",
       " '28': 217,\n",
       " 'days': 218,\n",
       " 'common': 219,\n",
       " 'february': 220,\n",
       " 'study': 221,\n",
       " 'living': 222,\n",
       " 'organisms': 223,\n",
       " 'called': 224,\n",
       " 'biology': 225,\n",
       " 'home': 226,\n",
       " 'great': 227,\n",
       " 'wall': 228,\n",
       " 'bees': 229,\n",
       " 'collect': 230,\n",
       " 'from': 231,\n",
       " 'flowers': 232,\n",
       " 'nectar': 233,\n",
       " 'opposite': 234,\n",
       " 'day': 235,\n",
       " 'south': 236,\n",
       " 'korea': 237,\n",
       " 'seoul': 238,\n",
       " 'bulb': 239,\n",
       " 'edison': 240,\n",
       " 'humans': 241,\n",
       " 'breathe': 242,\n",
       " 'survival': 243,\n",
       " 'oxygen': 244,\n",
       " '144': 245,\n",
       " '12': 246,\n",
       " 'pyramids': 247,\n",
       " 'giza': 248,\n",
       " 'egypt': 249,\n",
       " 'sea': 250,\n",
       " 'creature': 251,\n",
       " 'eight': 252,\n",
       " 'arms': 253,\n",
       " 'octopus': 254,\n",
       " 'holiday': 255,\n",
       " 'celebrated': 256,\n",
       " 'december': 257,\n",
       " '25': 258,\n",
       " 'christmas': 259,\n",
       " 'yen': 260,\n",
       " 'legs': 261,\n",
       " 'spider': 262,\n",
       " 'sport': 263,\n",
       " 'uses': 264,\n",
       " 'net,': 265,\n",
       " 'ball,': 266,\n",
       " 'hoop': 267,\n",
       " 'basketball': 268,\n",
       " 'kangaroos': 269,\n",
       " 'female': 270,\n",
       " 'minister': 271,\n",
       " 'uk': 272,\n",
       " 'margaretthatcher': 273,\n",
       " 'fastest': 274,\n",
       " 'animal': 275,\n",
       " 'cheetah': 276,\n",
       " 'periodic': 277,\n",
       " 'table': 278,\n",
       " 'spain': 279,\n",
       " 'madrid': 280,\n",
       " 'closest': 281,\n",
       " 'sun': 282,\n",
       " 'father': 283,\n",
       " 'computers': 284,\n",
       " 'charlesbabbage': 285,\n",
       " 'mexico': 286,\n",
       " 'mexicocity': 287,\n",
       " 'colors': 288,\n",
       " 'rainbow': 289,\n",
       " 'musical': 290,\n",
       " 'instrument': 291,\n",
       " 'black': 292,\n",
       " 'white': 293,\n",
       " 'keys': 294,\n",
       " 'piano': 295,\n",
       " 'americas': 296,\n",
       " '1492': 297,\n",
       " 'christophercolumbus': 298,\n",
       " 'disney': 299,\n",
       " 'character': 300,\n",
       " 'long': 301,\n",
       " 'nose': 302,\n",
       " 'grows': 303,\n",
       " 'it': 304,\n",
       " 'when': 305,\n",
       " 'lying': 306,\n",
       " 'pinocchio': 307,\n",
       " 'directed': 308,\n",
       " 'movie': 309,\n",
       " 'titanic': 310,\n",
       " 'jamescameron': 311,\n",
       " 'superhero': 312,\n",
       " 'also': 313,\n",
       " 'dark': 314,\n",
       " 'knight': 315,\n",
       " 'batman': 316,\n",
       " 'brasilia': 317,\n",
       " 'fruit': 318,\n",
       " 'king': 319,\n",
       " 'fruits': 320,\n",
       " 'mango': 321,\n",
       " 'eiffel': 322,\n",
       " 'tower': 323}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cce6db59-9762-4a42-8393-e9b7585799b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "324"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16dab7aa-796d-44a2-b351-df81190c9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting text to numerical indicies\n",
    "def text_to_indices(text, vocab):\n",
    "    indexed_text = []\n",
    "\n",
    "    for token in tokenize(text):\n",
    "\n",
    "        if token in vocab:\n",
    "            indexed_text.append(vocab[token])\n",
    "\n",
    "        else:\n",
    "            indexed_text.append(vocab[\"<UNK>\"])\n",
    "\n",
    "    return indexed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f4045df-fd7d-495e-94f8-74b438d3645f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_indices(\"what is ajay\", vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57e71894-0d29-4e26-a1b9-ee2c95acc61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'what': 1,\n",
       " 'is': 2,\n",
       " 'the': 3,\n",
       " 'capital': 4,\n",
       " 'of': 5,\n",
       " 'france': 6,\n",
       " 'paris': 7,\n",
       " 'germany': 8,\n",
       " 'berlin': 9,\n",
       " 'who': 10,\n",
       " 'wrote': 11,\n",
       " 'to': 12,\n",
       " 'kill': 13,\n",
       " 'a': 14,\n",
       " 'mockingbird': 15,\n",
       " 'harper-lee': 16,\n",
       " 'largest': 17,\n",
       " 'planet': 18,\n",
       " 'in': 19,\n",
       " 'our': 20,\n",
       " 'solar': 21,\n",
       " 'system': 22,\n",
       " 'jupiter': 23,\n",
       " 'boiling': 24,\n",
       " 'point': 25,\n",
       " 'water': 26,\n",
       " 'celsius': 27,\n",
       " '100': 28,\n",
       " 'painted': 29,\n",
       " 'mona': 30,\n",
       " 'lisa': 31,\n",
       " 'leonardo-da-vinci': 32,\n",
       " 'square': 33,\n",
       " 'root': 34,\n",
       " '64': 35,\n",
       " '8': 36,\n",
       " 'chemical': 37,\n",
       " 'symbol': 38,\n",
       " 'for': 39,\n",
       " 'gold': 40,\n",
       " 'au': 41,\n",
       " 'which': 42,\n",
       " 'year': 43,\n",
       " 'did': 44,\n",
       " 'world': 45,\n",
       " 'war': 46,\n",
       " 'ii': 47,\n",
       " 'end': 48,\n",
       " '1945': 49,\n",
       " 'longest': 50,\n",
       " 'river': 51,\n",
       " 'nile': 52,\n",
       " 'japan': 53,\n",
       " 'tokyo': 54,\n",
       " 'developed': 55,\n",
       " 'theory': 56,\n",
       " 'relativity': 57,\n",
       " 'albert-einstein': 58,\n",
       " 'freezing': 59,\n",
       " 'fahrenheit': 60,\n",
       " '32': 61,\n",
       " 'known': 62,\n",
       " 'as': 63,\n",
       " 'red': 64,\n",
       " 'mars': 65,\n",
       " 'author': 66,\n",
       " '1984': 67,\n",
       " 'george-orwell': 68,\n",
       " 'currency': 69,\n",
       " 'united': 70,\n",
       " 'kingdom': 71,\n",
       " 'pound': 72,\n",
       " 'india': 73,\n",
       " 'delhi': 74,\n",
       " 'discovered': 75,\n",
       " 'gravity': 76,\n",
       " 'newton': 77,\n",
       " 'how': 78,\n",
       " 'many': 79,\n",
       " 'continents': 80,\n",
       " 'are': 81,\n",
       " 'there': 82,\n",
       " 'on': 83,\n",
       " 'earth': 84,\n",
       " '7': 85,\n",
       " 'gas': 86,\n",
       " 'do': 87,\n",
       " 'plants': 88,\n",
       " 'use': 89,\n",
       " 'photosynthesis': 90,\n",
       " 'co2': 91,\n",
       " 'smallest': 92,\n",
       " 'prime': 93,\n",
       " 'number': 94,\n",
       " '2': 95,\n",
       " 'invented': 96,\n",
       " 'telephone': 97,\n",
       " 'alexander-graham-bell': 98,\n",
       " 'australia': 99,\n",
       " 'canberra': 100,\n",
       " 'ocean': 101,\n",
       " 'pacific-ocean': 102,\n",
       " 'speed': 103,\n",
       " 'light': 104,\n",
       " 'vacuum': 105,\n",
       " '299,792,458m/s': 106,\n",
       " 'language': 107,\n",
       " 'spoken': 108,\n",
       " 'brazil': 109,\n",
       " 'portuguese': 110,\n",
       " 'penicillin': 111,\n",
       " 'alexander-fleming': 112,\n",
       " 'canada': 113,\n",
       " 'ottawa': 114,\n",
       " 'mammal': 115,\n",
       " 'whale': 116,\n",
       " 'element': 117,\n",
       " 'has': 118,\n",
       " 'atomic': 119,\n",
       " '1': 120,\n",
       " 'hydrogen': 121,\n",
       " 'tallest': 122,\n",
       " 'mountain': 123,\n",
       " 'everest': 124,\n",
       " 'city': 125,\n",
       " 'big': 126,\n",
       " 'apple': 127,\n",
       " 'newyork': 128,\n",
       " 'planets': 129,\n",
       " 'starry': 130,\n",
       " 'night': 131,\n",
       " 'vangogh': 132,\n",
       " 'formula': 133,\n",
       " 'h2o': 134,\n",
       " 'italy': 135,\n",
       " 'rome': 136,\n",
       " 'country': 137,\n",
       " 'famous': 138,\n",
       " 'sushi': 139,\n",
       " 'was': 140,\n",
       " 'first': 141,\n",
       " 'person': 142,\n",
       " 'step': 143,\n",
       " 'moon': 144,\n",
       " 'armstrong': 145,\n",
       " 'main': 146,\n",
       " 'ingredient': 147,\n",
       " 'guacamole': 148,\n",
       " 'avocado': 149,\n",
       " 'sides': 150,\n",
       " 'does': 151,\n",
       " 'hexagon': 152,\n",
       " 'have': 153,\n",
       " '6': 154,\n",
       " 'china': 155,\n",
       " 'yuan': 156,\n",
       " 'pride': 157,\n",
       " 'and': 158,\n",
       " 'prejudice': 159,\n",
       " 'jane-austen': 160,\n",
       " 'iron': 161,\n",
       " 'fe': 162,\n",
       " 'hardest': 163,\n",
       " 'natural': 164,\n",
       " 'substance': 165,\n",
       " 'diamond': 166,\n",
       " 'continent': 167,\n",
       " 'by': 168,\n",
       " 'area': 169,\n",
       " 'asia': 170,\n",
       " 'president': 171,\n",
       " 'states': 172,\n",
       " 'george-washington': 173,\n",
       " 'bird': 174,\n",
       " 'its': 175,\n",
       " 'ability': 176,\n",
       " 'mimic': 177,\n",
       " 'sounds': 178,\n",
       " 'parrot': 179,\n",
       " 'longest-running': 180,\n",
       " 'animated': 181,\n",
       " 'tv': 182,\n",
       " 'show': 183,\n",
       " 'simpsons': 184,\n",
       " 'vaticancity': 185,\n",
       " 'most': 186,\n",
       " 'moons': 187,\n",
       " 'saturn': 188,\n",
       " 'romeo': 189,\n",
       " 'juliet': 190,\n",
       " 'shakespeare': 191,\n",
       " 'earths': 192,\n",
       " 'atmosphere': 193,\n",
       " 'nitrogen': 194,\n",
       " 'bones': 195,\n",
       " 'adult': 196,\n",
       " 'human': 197,\n",
       " 'body': 198,\n",
       " '206': 199,\n",
       " 'metal': 200,\n",
       " 'liquid': 201,\n",
       " 'at': 202,\n",
       " 'room': 203,\n",
       " 'temperature': 204,\n",
       " 'mercury': 205,\n",
       " 'russia': 206,\n",
       " 'moscow': 207,\n",
       " 'electricity': 208,\n",
       " 'benjamin-franklin': 209,\n",
       " 'second-largest': 210,\n",
       " 'land': 211,\n",
       " 'color': 212,\n",
       " 'ripe': 213,\n",
       " 'banana': 214,\n",
       " 'yellow': 215,\n",
       " 'month': 216,\n",
       " '28': 217,\n",
       " 'days': 218,\n",
       " 'common': 219,\n",
       " 'february': 220,\n",
       " 'study': 221,\n",
       " 'living': 222,\n",
       " 'organisms': 223,\n",
       " 'called': 224,\n",
       " 'biology': 225,\n",
       " 'home': 226,\n",
       " 'great': 227,\n",
       " 'wall': 228,\n",
       " 'bees': 229,\n",
       " 'collect': 230,\n",
       " 'from': 231,\n",
       " 'flowers': 232,\n",
       " 'nectar': 233,\n",
       " 'opposite': 234,\n",
       " 'day': 235,\n",
       " 'south': 236,\n",
       " 'korea': 237,\n",
       " 'seoul': 238,\n",
       " 'bulb': 239,\n",
       " 'edison': 240,\n",
       " 'humans': 241,\n",
       " 'breathe': 242,\n",
       " 'survival': 243,\n",
       " 'oxygen': 244,\n",
       " '144': 245,\n",
       " '12': 246,\n",
       " 'pyramids': 247,\n",
       " 'giza': 248,\n",
       " 'egypt': 249,\n",
       " 'sea': 250,\n",
       " 'creature': 251,\n",
       " 'eight': 252,\n",
       " 'arms': 253,\n",
       " 'octopus': 254,\n",
       " 'holiday': 255,\n",
       " 'celebrated': 256,\n",
       " 'december': 257,\n",
       " '25': 258,\n",
       " 'christmas': 259,\n",
       " 'yen': 260,\n",
       " 'legs': 261,\n",
       " 'spider': 262,\n",
       " 'sport': 263,\n",
       " 'uses': 264,\n",
       " 'net,': 265,\n",
       " 'ball,': 266,\n",
       " 'hoop': 267,\n",
       " 'basketball': 268,\n",
       " 'kangaroos': 269,\n",
       " 'female': 270,\n",
       " 'minister': 271,\n",
       " 'uk': 272,\n",
       " 'margaretthatcher': 273,\n",
       " 'fastest': 274,\n",
       " 'animal': 275,\n",
       " 'cheetah': 276,\n",
       " 'periodic': 277,\n",
       " 'table': 278,\n",
       " 'spain': 279,\n",
       " 'madrid': 280,\n",
       " 'closest': 281,\n",
       " 'sun': 282,\n",
       " 'father': 283,\n",
       " 'computers': 284,\n",
       " 'charlesbabbage': 285,\n",
       " 'mexico': 286,\n",
       " 'mexicocity': 287,\n",
       " 'colors': 288,\n",
       " 'rainbow': 289,\n",
       " 'musical': 290,\n",
       " 'instrument': 291,\n",
       " 'black': 292,\n",
       " 'white': 293,\n",
       " 'keys': 294,\n",
       " 'piano': 295,\n",
       " 'americas': 296,\n",
       " '1492': 297,\n",
       " 'christophercolumbus': 298,\n",
       " 'disney': 299,\n",
       " 'character': 300,\n",
       " 'long': 301,\n",
       " 'nose': 302,\n",
       " 'grows': 303,\n",
       " 'it': 304,\n",
       " 'when': 305,\n",
       " 'lying': 306,\n",
       " 'pinocchio': 307,\n",
       " 'directed': 308,\n",
       " 'movie': 309,\n",
       " 'titanic': 310,\n",
       " 'jamescameron': 311,\n",
       " 'superhero': 312,\n",
       " 'also': 313,\n",
       " 'dark': 314,\n",
       " 'knight': 315,\n",
       " 'batman': 316,\n",
       " 'brasilia': 317,\n",
       " 'fruit': 318,\n",
       " 'king': 319,\n",
       " 'fruits': 320,\n",
       " 'mango': 321,\n",
       " 'eiffel': 322,\n",
       " 'tower': 323}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8435ba8-b193-4a7b-b697-ffcd465f5301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "868e35c6-cab4-49ac-bc3e-7e2d7ac34744",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, vocab):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        numerical_question = text_to_indices(self.df.iloc[index][\"question\"], self.vocab)\n",
    "        numerical_answer = text_to_indices(self.df.iloc[index][\"answer\"], self.vocab)\n",
    "\n",
    "        return torch.tensor(numerical_question), torch.tensor(numerical_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaeae4d3-a7f8-4ee0-96e5-cb8309b6214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QADataset(df, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9838c09a-9e21-4507-a57e-7446472fd998",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9771db7e-1b77-4df4-82d0-49a773c52223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1,   2,   3,  92, 137,  19,   3,  45]]) tensor([[185]])\n",
      "tensor([[ 42,  86,  87, 241, 242,  19,  39, 243]]) tensor([[244]])\n",
      "tensor([[1, 2, 3, 4, 5, 6]]) tensor([[7]])\n",
      "tensor([[  1,   2,   3,  33,  34,   5, 245]]) tensor([[246]])\n",
      "tensor([[ 42,  18, 118,   3, 186, 187]]) tensor([[188]])\n",
      "tensor([[ 1,  2,  3, 69,  5,  3, 70, 71]]) tensor([[72]])\n",
      "tensor([[ 42, 117, 118,   3, 119,  94, 120]]) tensor([[121]])\n",
      "tensor([[  1,   2,   3,   4,   5, 286]]) tensor([[287]])\n",
      "tensor([[ 42,   2,   3, 210, 137, 168, 211, 169]]) tensor([[113]])\n",
      "tensor([[  1,   2,   3,  37, 133,   5,  26]]) tensor([[134]])\n",
      "tensor([[  1,   2,   3,  69,   5, 155]]) tensor([[156]])\n",
      "tensor([[  1,   2,   3,  37,  38,  39, 161]]) tensor([[162]])\n",
      "tensor([[ 42, 200,   2,  14, 201, 202, 203, 204]]) tensor([[205]])\n",
      "tensor([[  1,   2,   3, 221,   5, 222, 223, 224]]) tensor([[225]])\n",
      "tensor([[ 78,  79, 195,  81,  19,   3, 196, 197, 198]]) tensor([[199]])\n",
      "tensor([[ 10,  96,   3, 104, 239]]) tensor([[240]])\n",
      "tensor([[ 42, 299, 300, 118,  14, 301, 302, 158, 303, 304, 305, 306]]) tensor([[307]])\n",
      "tensor([[ 10,  29, 130, 131]]) tensor([[132]])\n",
      "tensor([[ 1,  2,  3, 69,  5, 53]]) tensor([[260]])\n",
      "tensor([[  1,   2,   3,   4,   5, 279]]) tensor([[280]])\n",
      "tensor([[  1,  87, 229, 230, 231, 232]]) tensor([[233]])\n",
      "tensor([[  1,   2,   3,   4,   5, 206]]) tensor([[207]])\n",
      "tensor([[ 42, 137,   2, 226,  12,   3, 227, 228]]) tensor([[155]])\n",
      "tensor([[ 42,  18,   2,   3, 281,  12,   3, 282]]) tensor([[205]])\n",
      "tensor([[ 78,  79, 288,  81,  19,  14, 289]]) tensor([[85]])\n",
      "tensor([[ 78,  79, 150, 151,  14, 152, 153]]) tensor([[154]])\n",
      "tensor([[ 42, 174,   2,  62,  39, 175, 176,  12, 177, 178]]) tensor([[179]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 53]]) tensor([[54]])\n",
      "tensor([[78, 79, 80, 81, 82, 83, 84]]) tensor([[85]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 99]]) tensor([[100]])\n",
      "tensor([[10, 29,  3, 30, 31]]) tensor([[32]])\n",
      "tensor([[ 42, 263, 264,  14, 265, 266, 158, 267]]) tensor([[268]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 175, 269]]) tensor([[99]])\n",
      "tensor([[10, 96,  3, 97]]) tensor([[98]])\n",
      "tensor([[ 42, 250, 251, 118, 252, 253]]) tensor([[254]])\n",
      "tensor([[ 42, 101,   2,   3,  17]]) tensor([[102]])\n",
      "tensor([[ 42, 137,   2,  62,  39,   3, 322, 323]]) tensor([[6]])\n",
      "tensor([[ 78,  79, 129,  81,  19,   3,  21,  22]]) tensor([[36]])\n",
      "tensor([[10, 75, 76]]) tensor([[77]])\n",
      "tensor([[ 1,  2,  3, 92, 93, 94]]) tensor([[95]])\n",
      "tensor([[ 42, 255,   2, 256,  83, 257, 258]]) tensor([[259]])\n",
      "tensor([[ 10,   2,  62,  63,   3, 283,   5, 284]]) tensor([[285]])\n",
      "tensor([[ 1,  2,  3, 37, 38, 39, 40]]) tensor([[41]])\n",
      "tensor([[10, 11, 12, 13, 14, 15]]) tensor([[16]])\n",
      "tensor([[  1,   2,   3, 180, 181, 182, 183]]) tensor([[184]])\n",
      "tensor([[ 42,   2,   3, 274, 211, 275]]) tensor([[276]])\n",
      "tensor([[  1,   2,   3,  17, 115,  83,  84]]) tensor([[116]])\n",
      "tensor([[42, 18,  2, 62, 63,  3, 64, 18]]) tensor([[65]])\n",
      "tensor([[  1,   2,   3,   4,   5, 113]]) tensor([[114]])\n",
      "tensor([[10,  2,  3, 66,  5, 67]]) tensor([[68]])\n",
      "tensor([[ 1,  2,  3, 24, 25,  5, 26, 19, 27]]) tensor([[28]])\n",
      "tensor([[ 1,  2,  3, 17, 18, 19, 20, 21, 22]]) tensor([[23]])\n",
      "tensor([[ 10, 140,   3, 141, 270,  93, 271,   5,   3, 272]]) tensor([[273]])\n",
      "tensor([[ 42, 137,   2, 138,  39, 139]]) tensor([[53]])\n",
      "tensor([[  1,   2,   3,   4,   5, 109]]) tensor([[317]])\n",
      "tensor([[ 10,  11, 189, 158, 190]]) tensor([[191]])\n",
      "tensor([[ 1,  2,  3, 33, 34,  5, 35]]) tensor([[36]])\n",
      "tensor([[  1,   2,   3, 122, 123,  19,   3,  45]]) tensor([[124]])\n",
      "tensor([[ 42, 137, 118,   3, 247,   5, 248]]) tensor([[249]])\n",
      "tensor([[ 10,  75, 208]]) tensor([[209]])\n",
      "tensor([[  1,   2,   3,   4,   5, 135]]) tensor([[136]])\n",
      "tensor([[ 10,  11, 157, 158, 159]]) tensor([[160]])\n",
      "tensor([[ 10,  75,   3, 296,  19, 297]]) tensor([[298]])\n",
      "tensor([[ 42, 125,   2,  62,  63,   3, 126, 127]]) tensor([[128]])\n",
      "tensor([[ 42, 318,   2,  62,  63,   3, 319,   5, 320]]) tensor([[321]])\n",
      "tensor([[ 78,  79, 261, 151,  14, 262, 153]]) tensor([[36]])\n",
      "tensor([[ 10, 308,   3, 309, 310]]) tensor([[311]])\n",
      "tensor([[  1,   2,   3, 234,   5, 235]]) tensor([[131]])\n",
      "tensor([[  1,   2,   3,   4,   5, 236, 237]]) tensor([[238]])\n",
      "tensor([[  1,   2,   3, 103,   5, 104,  19, 105]]) tensor([[106]])\n",
      "tensor([[ 1,  2,  3,  4,  5, 73]]) tensor([[74]])\n",
      "tensor([[  1,   2,   3, 146,  86,  19, 192, 193]]) tensor([[194]])\n",
      "tensor([[ 1,  2,  3, 59, 25,  5, 26, 19, 60]]) tensor([[61]])\n",
      "tensor([[  1,   2,   3, 212,   5,  14, 213, 214]]) tensor([[215]])\n",
      "tensor([[ 10, 140,   3, 141, 142,  12, 143,  83,   3, 144]]) tensor([[145]])\n",
      "tensor([[ 10,  75, 111]]) tensor([[112]])\n",
      "tensor([[ 1,  2,  3, 50, 51, 19,  3, 45]]) tensor([[52]])\n",
      "tensor([[1, 2, 3, 4, 5, 8]]) tensor([[9]])\n",
      "tensor([[ 42, 290, 291, 118, 292, 158, 293, 294]]) tensor([[295]])\n",
      "tensor([[42, 43, 44, 45, 46, 47, 48]]) tensor([[49]])\n",
      "tensor([[  1,   2,   3, 146, 147,  19, 148]]) tensor([[149]])\n",
      "tensor([[ 10, 140,   3, 141, 171,   5,   3,  70, 172]]) tensor([[173]])\n",
      "tensor([[ 42, 216, 118, 217, 218,  19,  14, 219,  43]]) tensor([[220]])\n",
      "tensor([[  1,   2,   3, 141, 117,  83,   3, 277, 278]]) tensor([[121]])\n",
      "tensor([[  1,   2,   3, 163, 164, 165,  83,  84]]) tensor([[166]])\n",
      "tensor([[ 42, 107,   2, 108,  19, 109]]) tensor([[110]])\n",
      "tensor([[10, 55,  3, 56,  5, 57]]) tensor([[58]])\n",
      "tensor([[42, 86, 87, 88, 89, 39, 90]]) tensor([[91]])\n",
      "tensor([[ 42, 167,   2,   3,  17, 168, 169]]) tensor([[170]])\n",
      "tensor([[ 42, 312,   2, 313,  62,  63,   3, 314, 315]]) tensor([[316]])\n"
     ]
    }
   ],
   "source": [
    "for question, answer in dataloader:\n",
    "    print(question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9de78419-2e92-4196-8f3a-6e47d20331d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6869af0b-6c45-4056-8abf-1ac507bfe1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim = 50)\n",
    "        self.rnn = nn.RNN(50, 64, batch_first=True)\n",
    "        self.fc = nn.Linear(64, vocab_size)\n",
    "\n",
    "    def forward(self, question):\n",
    "        emdedded_question = self.embeddings(question)\n",
    "        hidden, final = self.rnn(emdedded_question)\n",
    "        output = self.fc(final.squeeze(0))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42e8cbb0-9b8d-4cf2-b7fc-1d9bce5f3eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  2,  3, 17, 18, 19, 20, 21, 22])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92f4e695-abbd-4eff-8a17-97655274c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = nn.Embedding(324, embedding_dim = 50) \n",
    "# it generates the [6,50] embeddings because our vestor has 6 words and we want the every word should be of 50 dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d5d879a3-490d-48ac-938b-7135da7110d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 50])\n"
     ]
    }
   ],
   "source": [
    "print(x(dataset[3][0]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8bc81a41-01d2-4759-9823-0a693fef21b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f2610bf-0669-4144-afe6-c58c0052842a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 50])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0228e+00,  1.2902e+00,  4.7001e-01,  1.3188e+00, -1.2374e-01,\n",
       "         -6.7735e-01,  1.5142e+00, -5.9998e-01, -7.1084e-01, -1.2694e+00,\n",
       "         -9.3589e-01, -5.0032e-01,  3.6159e-01, -4.5901e-03,  2.1516e-02,\n",
       "          1.6140e+00,  7.3907e-01,  2.8858e-01, -5.2929e-01,  4.8753e-01,\n",
       "         -6.3531e-01,  1.1588e+00,  9.8738e-01, -1.3891e-01, -4.0180e-01,\n",
       "          1.5329e+00, -1.2546e-01, -2.8893e-01,  8.5959e-01,  4.3177e-01,\n",
       "         -3.6928e-01, -9.7027e-01,  1.4610e+00, -5.5727e-01,  1.2439e+00,\n",
       "         -8.5883e-01,  7.0716e-01,  2.6460e-01,  1.0229e-01, -2.4431e+00,\n",
       "         -5.9472e-01,  5.1581e-01, -7.5611e-01, -1.5905e+00,  7.4599e-01,\n",
       "         -1.0506e+00, -1.4279e-01,  4.7230e-01, -1.7027e+00, -5.2122e-01],\n",
       "        [ 1.6101e-01,  1.6037e+00, -5.7788e-01,  6.5995e-02, -7.7933e-01,\n",
       "          4.1706e-01, -9.6231e-01,  2.1237e+00,  1.0935e+00, -4.9366e-01,\n",
       "          3.2673e-01, -1.0456e+00,  9.4265e-01,  5.5226e-01, -8.9401e-01,\n",
       "          6.8184e-01,  7.2779e-01, -1.1558e+00, -7.8134e-01, -1.3933e-01,\n",
       "          1.6869e+00,  1.3276e-01, -1.7336e+00,  3.0737e-01, -4.5519e-01,\n",
       "         -8.2367e-01, -5.2523e-01, -1.5031e+00,  1.9090e+00,  7.9429e-01,\n",
       "          1.0643e-02,  1.4139e+00,  1.0991e+00,  8.8504e-01,  1.0036e+00,\n",
       "          5.2621e-01,  7.3868e-01,  2.2970e+00,  1.8807e+00, -1.4597e+00,\n",
       "          1.1254e+00, -9.0287e-01,  4.5858e-01, -9.4625e-02, -5.1715e-01,\n",
       "         -1.0009e+00, -2.4682e-02, -5.6450e-01,  8.4350e-01, -4.2879e-01],\n",
       "        [ 1.7971e+00,  3.2697e-01, -7.5963e-01, -1.5183e+00,  1.8883e+00,\n",
       "         -9.5033e-01,  1.3290e+00, -2.5683e+00, -1.6184e+00, -5.0808e-01,\n",
       "         -1.5955e+00, -1.2760e+00, -7.0385e-01, -2.2938e+00,  6.0262e-01,\n",
       "          4.8259e-01,  3.9600e-01,  4.9803e-01, -1.1910e+00, -1.2236e+00,\n",
       "         -2.8741e-01, -4.7371e-01, -6.7100e-01,  8.7982e-01,  4.7030e-01,\n",
       "          1.0346e+00,  6.7157e-01, -1.5020e-01,  7.8437e-01,  3.5632e-01,\n",
       "         -4.0194e-01, -6.6924e-01, -9.1446e-01,  1.8495e+00, -1.7407e+00,\n",
       "          3.5736e-01, -5.4201e-01,  7.7248e-01,  1.0385e+00,  7.4906e-01,\n",
       "          1.1678e-01, -7.5951e-01, -1.6829e-02,  2.2432e+00,  3.2026e-01,\n",
       "         -1.0365e+00,  5.1635e-01, -1.8859e-01, -7.5499e-01, -9.7284e-01],\n",
       "        [-5.4323e-02, -8.2555e-01, -9.6079e-01, -1.1294e-01, -1.8585e+00,\n",
       "         -1.1770e+00,  1.0832e-01, -7.1385e-02, -8.8195e-01,  9.3166e-01,\n",
       "          1.6470e-01,  6.4476e-01,  4.6675e-01, -1.7521e+00, -2.2069e+00,\n",
       "          9.7686e-02,  5.0208e-01,  1.7890e+00, -1.8794e+00,  2.8728e+00,\n",
       "          9.4848e-01, -8.6929e-01,  8.1992e-01, -7.0661e-01, -2.1665e-01,\n",
       "         -1.2003e+00, -9.4611e-02, -1.2449e+00,  4.2891e-01,  5.3290e-01,\n",
       "         -1.2510e-02, -1.6807e+00,  1.4953e+00,  1.7065e-01,  1.6645e-01,\n",
       "         -1.3347e+00,  1.8958e+00, -1.9653e+00,  1.6129e-01,  5.6563e-02,\n",
       "         -8.3300e-02,  7.8103e-01, -8.6146e-01,  1.5218e+00, -1.2288e+00,\n",
       "          9.7856e-02, -2.4366e-01,  5.6924e-01, -1.4544e-01,  1.7158e+00],\n",
       "        [-9.8385e-01, -1.0368e-01,  4.5473e-01, -2.0596e-01,  1.4688e-01,\n",
       "         -1.4867e+00,  1.4782e+00, -1.2438e+00,  1.6727e+00,  3.0565e-01,\n",
       "         -5.7647e-02, -6.6335e-01,  1.0604e+00, -7.2078e-01, -8.4134e-01,\n",
       "          1.1865e+00,  7.2731e-01,  5.9363e-01, -5.6091e-01,  2.9322e-01,\n",
       "         -3.6694e-01,  1.7225e-01, -1.0922e+00, -3.1703e-01, -1.1406e+00,\n",
       "         -1.0223e+00,  5.4252e-01, -7.3314e-01, -1.2838e+00, -9.4658e-01,\n",
       "          1.7021e+00, -1.3103e+00,  2.9049e+00,  4.7814e-01,  1.9980e+00,\n",
       "         -3.2700e-01, -1.1149e-01,  3.6082e-01, -7.8945e-01,  7.3164e-01,\n",
       "         -9.8181e-01, -7.9294e-01,  1.1379e+00, -5.2494e-01, -1.8629e-01,\n",
       "         -7.9526e-02, -4.3372e-01,  6.6814e-01,  2.0940e+00, -1.2145e+00],\n",
       "        [ 3.4751e+00, -1.9204e+00, -3.5345e-01, -2.9241e-01,  2.8674e-03,\n",
       "         -1.9129e+00, -1.3854e+00,  8.8770e-01, -7.2720e-02,  3.0764e-01,\n",
       "         -8.8320e-01,  1.6476e+00, -1.0825e+00,  8.5672e-01, -1.1941e-01,\n",
       "          1.6478e+00, -6.9271e-01, -7.5826e-01,  5.2209e-01,  8.3828e-01,\n",
       "          7.8922e-01, -4.9834e-01, -1.0444e+00,  6.7685e-01,  3.4992e-01,\n",
       "         -6.6057e-02, -7.6358e-01,  9.9865e-01, -5.8913e-02, -6.4967e-03,\n",
       "         -1.2364e+00, -7.0438e-01, -4.3578e-01,  1.2648e+00,  1.9102e-01,\n",
       "         -5.5697e-02, -2.2372e-02,  2.1404e-01,  1.0534e+00, -8.2018e-01,\n",
       "         -1.0367e+00,  3.1796e-01,  2.2641e+00, -1.9951e-01, -3.5797e-01,\n",
       "          1.1963e+00,  5.4796e-01, -1.2834e+00,  3.9300e-01, -6.6559e-01]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x(dataset[0][0]).shape)\n",
    "a = x(dataset[0][0])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b261c004-c939-4b09-ba1c-7393033015ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = nn.RNN(50, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a4903af-900e-42d6-b612-516e5ec13761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0343,  0.5501, -0.7140,  0.2809,  0.4558,  0.3066, -0.0506, -0.5815,\n",
       "           0.4267,  0.7010,  0.1682,  0.3963,  0.1139, -0.0600, -0.0263, -0.5276,\n",
       "           0.3610, -0.2113,  0.3841,  0.3248, -0.0533,  0.5559, -0.5290,  0.0324,\n",
       "           0.3452, -0.7156, -0.1289, -0.2774,  0.1417, -0.2848,  0.3456,  0.2753,\n",
       "          -0.0347, -0.6000, -0.5078, -0.1551,  0.5495,  0.4815,  0.0849, -0.2114,\n",
       "          -0.5026,  0.4782,  0.6379, -0.2080,  0.8566, -0.1086,  0.3251,  0.4758,\n",
       "          -0.3609, -0.5262, -0.3731, -0.3837, -0.7279,  0.4402,  0.6754,  0.2791,\n",
       "          -0.4241,  0.5992,  0.0162,  0.6337, -0.5592, -0.4798, -0.4925,  0.4369],\n",
       "         [-0.0712,  0.2222,  0.4529, -0.2210,  0.7302, -0.3973, -0.0742, -0.6222,\n",
       "          -0.2711, -0.3236,  0.0349,  0.0576,  0.6769, -0.1995,  0.7648, -0.4688,\n",
       "           0.7781, -0.1518,  0.5099,  0.1968, -0.6103, -0.1208,  0.0033, -0.3294,\n",
       "          -0.0727,  0.2108,  0.3317, -0.5843,  0.5853, -0.1612, -0.2516,  0.6024,\n",
       "          -0.7264,  0.0967, -0.5182, -0.4266,  0.0863, -0.0835, -0.1063, -0.6459,\n",
       "          -0.1211,  0.4939,  0.0600,  0.4326, -0.0058, -0.5825, -0.3764, -0.1711,\n",
       "           0.7882, -0.8148,  0.5270, -0.6789, -0.1827, -0.0892,  0.1516, -0.3270,\n",
       "           0.0094,  0.7304,  0.1087, -0.6512,  0.6913, -0.6149,  0.5779,  0.7047],\n",
       "         [-0.7423, -0.1530,  0.4691,  0.5971,  0.4933,  0.2537, -0.2023, -0.0029,\n",
       "           0.9176,  0.2844, -0.3753,  0.2937,  0.0635,  0.7448,  0.7710, -0.0840,\n",
       "          -0.4369,  0.1488, -0.8577,  0.8122,  0.8559, -0.5848,  0.0878,  0.1684,\n",
       "          -0.4757,  0.0949,  0.8338, -0.3813,  0.2861,  0.3705,  0.0296,  0.6577,\n",
       "          -0.3105, -0.1913, -0.7338, -0.7598, -0.3508,  0.5220, -0.7566,  0.5708,\n",
       "          -0.2364, -0.6776,  0.0831, -0.5587,  0.0460, -0.3633, -0.4866, -0.4044,\n",
       "          -0.6653, -0.6194, -0.1770,  0.5609,  0.0050,  0.3752,  0.3760,  0.6423,\n",
       "           0.0203,  0.1122, -0.4471, -0.5165, -0.6470,  0.5707, -0.8533,  0.4172],\n",
       "         [ 0.5851, -0.7438, -0.6689, -0.8123,  0.8356, -0.6120, -0.0666,  0.6972,\n",
       "           0.1763,  0.4859,  0.4280,  0.6179,  0.4768,  0.7784,  0.0788, -0.2737,\n",
       "          -0.4130,  0.3273,  0.1912, -0.8632, -0.7790,  0.3316, -0.0812,  0.0180,\n",
       "           0.4383,  0.0226,  0.5390,  0.1596, -0.4228,  0.7233,  0.3026, -0.5635,\n",
       "          -0.5334,  0.8025,  0.2326, -0.2246, -0.7565, -0.5159, -0.2951,  0.5370,\n",
       "          -0.1597, -0.3337,  0.0407, -0.7262, -0.5075,  0.1092, -0.4469, -0.2729,\n",
       "           0.8151,  0.4277, -0.6928,  0.5117, -0.8303, -0.5059,  0.6302,  0.4950,\n",
       "           0.3018,  0.4344,  0.0616,  0.3518, -0.5876, -0.6669,  0.4745,  0.1165],\n",
       "         [ 0.0927, -0.2207, -0.4987, -0.7804,  0.7825, -0.0853,  0.4087, -0.5157,\n",
       "          -0.1232,  0.3103,  0.0384,  0.9291,  0.2312,  0.7071,  0.1063,  0.4921,\n",
       "           0.6043, -0.1181,  0.0482, -0.6092, -0.8612,  0.5409, -0.8944,  0.7016,\n",
       "           0.5121, -0.1425, -0.2037, -0.4821, -0.3117, -0.3217, -0.6960,  0.2141,\n",
       "          -0.4771, -0.0206,  0.0781, -0.5646, -0.1932,  0.4593,  0.4543, -0.3066,\n",
       "          -0.7673,  0.2015, -0.1277, -0.8695,  0.4582,  0.4193,  0.2765,  0.7522,\n",
       "           0.6653, -0.4507,  0.4829,  0.4932,  0.0286, -0.1320,  0.7890,  0.8219,\n",
       "          -0.8969,  0.0312,  0.3019,  0.4665, -0.3408, -0.8954, -0.6697, -0.3672],\n",
       "         [-0.0653,  0.7844, -0.0983, -0.1187, -0.3466,  0.2075, -0.0705, -0.1248,\n",
       "           0.8240, -0.2533, -0.2789,  0.3348,  0.9333,  0.3137,  0.4003, -0.9266,\n",
       "           0.5123, -0.1330, -0.5933,  0.4953,  0.3079, -0.0888, -0.7469, -0.4295,\n",
       "          -0.0318, -0.8472,  0.3981,  0.0561, -0.3868, -0.1455, -0.9005,  0.9523,\n",
       "          -0.5909, -0.1496,  0.3959, -0.6945, -0.2447,  0.6222, -0.8201, -0.1025,\n",
       "           0.3245, -0.2240,  0.1117, -0.3896, -0.4365, -0.6073,  0.5438,  0.0941,\n",
       "           0.8265,  0.1817, -0.1157, -0.0634, -0.6327,  0.4317, -0.5499,  0.3049,\n",
       "           0.3014,  0.6534, -0.4799,  0.4982, -0.3884,  0.6298, -0.6584,  0.4817]],\n",
       "        grad_fn=<SqueezeBackward1>),\n",
       " tensor([[-0.0653,  0.7844, -0.0983, -0.1187, -0.3466,  0.2075, -0.0705, -0.1248,\n",
       "           0.8240, -0.2533, -0.2789,  0.3348,  0.9333,  0.3137,  0.4003, -0.9266,\n",
       "           0.5123, -0.1330, -0.5933,  0.4953,  0.3079, -0.0888, -0.7469, -0.4295,\n",
       "          -0.0318, -0.8472,  0.3981,  0.0561, -0.3868, -0.1455, -0.9005,  0.9523,\n",
       "          -0.5909, -0.1496,  0.3959, -0.6945, -0.2447,  0.6222, -0.8201, -0.1025,\n",
       "           0.3245, -0.2240,  0.1117, -0.3896, -0.4365, -0.6073,  0.5438,  0.0941,\n",
       "           0.8265,  0.1817, -0.1157, -0.0634, -0.6327,  0.4317, -0.5499,  0.3049,\n",
       "           0.3014,  0.6534, -0.4799,  0.4982, -0.3884,  0.6298, -0.6584,  0.4817]],\n",
       "        grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0676fdc7-b765-4918-9ab6-671d41bdcecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 64])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden states\n",
    "y(a)[0].shape\n",
    "# these are the outputs of intermediate stage in RNNs\n",
    "# outputs like [o1, o2, o3, o4, o5, o6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e0e62a9-451f-4155-852c-47d18a7f3cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final output\n",
    "b = y(a)[1]\n",
    "y(a)[1].shape\n",
    "# and this is the final output and we can see that it is equal of last from intermediate stage \n",
    "# final output is [o6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6fe03181-5e45-4b47-900e-662e954766d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = nn.Linear(64, 324)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dab388a2-56db-453d-8ae4-b4eaa617df8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 324])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0432,  0.3001,  0.6356,  0.0551,  0.6062, -0.6885, -0.1355, -0.2815,\n",
       "          0.2893,  0.0504,  0.2582,  0.2666, -0.3728,  0.1008,  0.3112,  0.1854,\n",
       "         -0.0649,  0.4106, -0.0940,  0.7339, -0.0094,  0.1674, -0.1012,  0.0696,\n",
       "          0.6726,  0.3734, -0.0992,  0.2714,  0.1761, -0.2645, -0.3378, -0.1272,\n",
       "         -0.1231,  0.5825,  0.1101, -0.1220, -0.2049,  0.2044,  0.1554,  0.7725,\n",
       "         -0.0603, -0.0491,  0.2144, -0.3450, -0.5859, -0.5520, -0.3242, -0.5169,\n",
       "         -0.1089, -0.1414, -0.0416, -0.2338,  0.0865, -0.0989,  0.1894, -0.1820,\n",
       "         -0.0528,  0.5004, -0.1386,  0.0953,  0.0532,  0.4272,  0.0981, -0.6189,\n",
       "          0.3088, -0.1921, -0.2097, -0.0521,  0.1431,  0.0316, -0.3511, -0.5057,\n",
       "         -0.1556, -0.0972,  0.3879, -0.2747, -0.0440, -0.2688,  0.0074,  0.1298,\n",
       "         -0.0070, -0.1553, -0.3648, -0.8632, -0.0346, -0.3082,  0.2869,  0.0899,\n",
       "         -0.2173,  0.0471, -0.2480, -0.3319,  0.3620, -0.1030,  0.1714,  0.1729,\n",
       "         -0.1494, -0.2754, -0.1569,  0.1110, -0.3444,  0.4615,  0.0554, -0.1944,\n",
       "          0.1831,  0.3055,  0.2144,  0.1553,  0.3803, -0.0043,  0.1793, -0.2153,\n",
       "          0.1250,  0.3485,  0.4622,  0.1435,  0.2415, -0.2902,  0.1728, -0.4561,\n",
       "          0.1451,  0.4642,  0.1812,  0.4600,  0.0880,  0.2874, -0.0954, -0.3837,\n",
       "          0.3628, -0.1776,  0.1115, -0.4397, -0.0594,  0.3080, -0.0512,  0.1992,\n",
       "          0.2489, -0.1249,  0.2861,  0.0804, -0.3314, -0.4433,  0.1717, -0.3880,\n",
       "         -0.0111,  0.7446,  0.0537,  0.0660, -0.2513, -0.1267, -0.2191,  0.2040,\n",
       "         -0.2192, -0.3799,  0.4323, -0.0064, -0.3504,  0.2390,  0.1587,  0.2385,\n",
       "         -0.0435, -0.2420,  0.0087,  0.1579,  0.2323, -0.2680,  0.6422, -0.2229,\n",
       "         -0.0372,  0.4600,  0.1179, -0.0311,  0.2572, -0.3400,  0.1428,  0.3459,\n",
       "         -0.6714,  0.3451, -0.2059,  0.1314, -0.5209,  0.3883, -0.4219, -0.2705,\n",
       "          0.0388, -0.0840,  0.1315, -0.4657,  0.1668,  0.3156,  0.2176,  0.0486,\n",
       "          0.3634, -0.0968,  0.1945,  0.2792, -0.1174,  0.0913,  0.7311,  0.2153,\n",
       "         -0.3826,  0.5188,  0.0406,  0.3945, -0.3425, -0.2734,  0.2310, -0.1411,\n",
       "         -0.2052, -0.0317, -0.6089,  0.0980,  0.1025, -0.3390,  0.0137, -0.3607,\n",
       "          0.0118, -0.2907,  0.0840,  0.3075, -0.3760,  0.3839, -0.2368, -0.4452,\n",
       "         -0.1496,  0.4289,  0.0135,  0.3684, -0.2874, -0.1208,  0.2429, -0.0181,\n",
       "          0.0124, -0.3909, -0.4617,  0.8655, -0.0605, -0.1079, -0.1043, -0.0266,\n",
       "         -0.1257, -0.2032, -0.4155, -0.4926, -0.3784, -0.1960, -0.1943, -0.6058,\n",
       "          0.1454,  0.1392,  0.1348,  0.3077,  0.4070,  0.6023,  0.0090, -0.0445,\n",
       "          0.2730, -0.1312,  0.1108, -0.1023, -0.0791,  0.1404, -0.7583, -0.1594,\n",
       "         -0.1361, -0.3974,  0.1426, -0.1277, -0.2120,  0.3285,  0.2665, -0.0977,\n",
       "         -0.1939,  0.1625, -0.2306,  0.1612, -0.1941,  0.3337,  0.4065, -0.1988,\n",
       "         -0.0600,  0.2680,  0.4527,  0.0863, -0.3849, -0.5712, -0.3390,  0.1889,\n",
       "         -0.1378,  0.0880,  0.4138, -0.1725,  0.0973,  0.2318, -0.4101, -0.2533,\n",
       "         -0.0315, -0.0127,  0.4045,  0.2248,  0.1263,  0.3045, -0.3832, -0.0053,\n",
       "         -0.3089, -0.4625,  0.0214, -0.1614,  0.1667, -0.0048,  0.1535, -0.1493,\n",
       "          0.4150, -0.0248,  0.4195,  0.5301,  0.2739, -0.1575,  0.5298, -0.1392,\n",
       "         -0.5970,  0.1281,  0.0665,  0.5486]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(z(b).shape)\n",
    "z(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "79f709a0-5872-43e1-8752-3066459e1cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of a: torch.Size([1, 6])\n",
      "shape of b: torch.Size([1, 6, 50])\n",
      "shape of c: torch.Size([1, 6, 64])\n",
      "shape of d: torch.Size([1, 6, 64])\n",
      "shape of e: torch.Size([1, 6, 324])\n"
     ]
    }
   ],
   "source": [
    "# debugging\n",
    "x = nn.Embedding(324, embedding_dim=50)\n",
    "y = nn.RNN(50, 64)\n",
    "z = nn.Linear(64, 324)\n",
    "\n",
    "a = dataset[0][0].reshape(1,6)\n",
    "print(\"shape of a:\", a.shape)\n",
    "b = x(a)\n",
    "print(\"shape of b:\", b.shape)\n",
    "c, d = y(b)\n",
    "print(\"shape of c:\", c.shape)\n",
    "print(\"shape of d:\", d.shape)\n",
    "\n",
    "e = z(d)\n",
    "\n",
    "print(\"shape of e:\", e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ef2c0fc3-c468-4fa1-ad59-3d17fba8e2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of a: torch.Size([1, 6])\n",
      "shape of b: torch.Size([1, 6, 50])\n",
      "shape of c: torch.Size([1, 6, 64])\n",
      "shape of d: torch.Size([1, 1, 64])\n",
      "shape of e: torch.Size([1, 324])\n"
     ]
    }
   ],
   "source": [
    "x = nn.Embedding(324, embedding_dim=50)\n",
    "y = nn.RNN(50, 64, batch_first=True)\n",
    "z = nn.Linear(64, 324)\n",
    "\n",
    "a = dataset[0][0].reshape(1,6)\n",
    "print(\"shape of a:\", a.shape)\n",
    "b = x(a)\n",
    "print(\"shape of b:\", b.shape)\n",
    "c, d = y(b)\n",
    "print(\"shape of c:\", c.shape)\n",
    "print(\"shape of d:\", d.shape)\n",
    "\n",
    "e = z(d.squeeze(0))\n",
    "\n",
    "print(\"shape of e:\", e.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "62ee7fad-aeda-4668-806a-ee197fa0a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a5c4a243-e7d5-4970-abf0-11f3f7e91a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRNN(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "91777c39-317c-4331-8058-274a991b8b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterian = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cb36de36-4148-4907-800f-fac94644a080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss: 10.302537\n",
      "Epoch: 2 | loss: 9.025960\n",
      "Epoch: 3 | loss: 7.976411\n",
      "Epoch: 4 | loss: 7.127881\n",
      "Epoch: 5 | loss: 6.389466\n",
      "Epoch: 6 | loss: 5.756176\n",
      "Epoch: 7 | loss: 5.204326\n",
      "Epoch: 8 | loss: 4.739720\n",
      "Epoch: 9 | loss: 4.309742\n",
      "Epoch: 10 | loss: 3.941969\n",
      "Epoch: 11 | loss: 3.625783\n",
      "Epoch: 12 | loss: 3.336977\n",
      "Epoch: 13 | loss: 3.076548\n",
      "Epoch: 14 | loss: 2.850929\n",
      "Epoch: 15 | loss: 2.638151\n",
      "Epoch: 16 | loss: 2.450876\n",
      "Epoch: 17 | loss: 2.279607\n",
      "Epoch: 18 | loss: 2.120859\n",
      "Epoch: 19 | loss: 1.981034\n",
      "Epoch: 20 | loss: 1.850931\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for question, answer in dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        output = model(question)\n",
    "        # print(output.shape)\n",
    "\n",
    "        # loss \n",
    "        loss = criterian(output, answer[0])\n",
    "\n",
    "         #backward\n",
    "        loss.backward()\n",
    "\n",
    "        # update\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "    print(f\"Epoch: {epoch+1} | loss: {total_loss:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d9434403-0e36-4064-8c01-111fdb55fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, question, thresold=0.5):\n",
    "\n",
    "    # convert question to numbers\n",
    "    numerical_question = text_to_indices(question, vocab)\n",
    "\n",
    "    # convert to tensor\n",
    "    question_tensor = torch.tensor(numerical_question).unsqueeze(0)\n",
    "\n",
    "    # send to model\n",
    "    output = model(question_tensor) # logits\n",
    "\n",
    "    # convert logits to probs\n",
    "    probs = nn.functional.softmax(output, dim=1)\n",
    "\n",
    "    # find index of max probs\n",
    "    value, index = torch.max(probs, dim=1)\n",
    "\n",
    "    if value < thresold:\n",
    "        print(\"I dont know\")\n",
    "\n",
    "    print(list(vocab.keys())[index])\n",
    "    print(index, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a80de70c-124e-4165-ab0d-0baf2210cdc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupiter\n",
      "tensor([23]) tensor([0.9644], grad_fn=<MaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "predict(model, \"What is largest planet in our solar system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "1d104985-cc7c-4b20-981c-b9cffed12213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jupiter'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vocab.keys())[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce21b491-b8b6-438e-9d02-de4f46c68921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
